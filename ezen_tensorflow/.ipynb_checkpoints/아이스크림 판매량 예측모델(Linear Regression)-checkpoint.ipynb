{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " # 쥬피터에서만 쓸수 있음 다른곳에서는 이 라인 제거 필요 : 그림 그릴때 여기서 나타낼지 아닌지\n",
    "%matplotlib inline  \n",
    "#레티나 디스플레이용임..조금이라도 고화질의 레티나 디스플레이에서 쓸때는 필요. 깃허브에서 레티나까지 고려하면 성의있게 짠 코드\n",
    "%config InlineBackend.figure_format = 'retina'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([20,21,22,23,24,25,26])\n",
    "Y = np.array([15,16,21,33,42,60,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목적함수 : Cost Function , Loss Function\n",
    "def cost_f(a,b):\n",
    "    return np.sum((a * X + b - Y)**2)\n",
    "\n",
    "# 함수는 수학적 아이디어작성하기 파일에서 구했음(참고하세요) : x의 편미분과 y의 편미분\n",
    "def diff_a(a,b):\n",
    "    return np.sum(2*X*(X*a - Y + b))\n",
    "\n",
    "def diff_b(a,b):\n",
    "    return np.sum(2*X*a - 2*Y + 2*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization, minimization\n",
    "\n",
    "def gradient_descent1D(func, diff_a, a0, learning_rate=0.001, Maxlter=10,verbose=True):\n",
    "    \n",
    "    paths=[]\n",
    "    for i in range(Maxlter):\n",
    "        a1 = a0 - learning_rate * diff_a(a0)\n",
    "        if verbose:\n",
    "            print(i, a1, func(a1))\n",
    "        a0 =  a1\n",
    "        paths.append(a0)\n",
    "        \n",
    "    return (a0, func(a0), paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a0 = [[a]\n",
    "#       [b]]\n",
    "\n",
    "\n",
    "def gradient_descent2D(func, diff_a, diff_b, a0, learning_rate=0.01, Maxlter=10, verbose=True):\n",
    "    paths=[a0]\n",
    "    for i in range(Maxlter):\n",
    "        diff = np.array([diff_a(*a0), diff_b(*a0)])  #아규먼트에 *가 붙었으므로 리스트형태로 전달\n",
    "        a1 = a0 - learning_rate * diff\n",
    "        if verbose:\n",
    "            print(i, a1, func(*a1))\n",
    "        a0 =  a1\n",
    "        paths.append(a0)\n",
    "        \n",
    "    return(a0,func(*a0),paths)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10   2]\n",
      "0 [8593.4  371.4] 276446242359.7999\n",
      "1 [-6414554.96  -276805.84] 1.5409000133055315e+17\n",
      "2 [4.78904071e+09 2.06659442e+08] 8.588913510579295e+22\n",
      "3 [-3.57544757e+12 -1.54289775e+11] 4.787425183673176e+28\n",
      "4 [2.66939166e+15 1.15191128e+14] 2.668491173072984e+34\n",
      "5 [-1.99293982e+18 -8.60004879e+16] 1.4874060413629125e+40\n",
      "6 [1.48790797e+21 6.42070624e+19] 8.290740303761836e+45\n",
      "7 [-1.11085649e+24 -4.79363194e+22] 4.6212246604456235e+51\n",
      "8 [8.29353803e+26 3.57887534e+25] 2.5758516827044782e+57\n",
      "9 [-6.19186852e+29 -2.67195080e+28] 1.4357691691733677e+63\n",
      "[-6.19186852e+29 -2.67195080e+28]\n"
     ]
    }
   ],
   "source": [
    "# 구하고자 하는 값은 벡터값 a0임\n",
    "a=-10\n",
    "b=2\n",
    "a0=np.array([a,b])\n",
    "print(a0)\n",
    "\n",
    "result, _, _=gradient_descent2D(cost_f, diff_a, diff_b, a0, learning_rate=0.1, Maxlter=10)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
